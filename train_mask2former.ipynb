{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b692e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 56/56 [78.7ms elapsed, 0s remaining, 711.6 samples/s]  \n"
     ]
    }
   ],
   "source": [
    "#get training dataset\n",
    "import fiftyone as fo\n",
    "from fiftyone.utils.data.importers import ImageSegmentationDirectoryImporter\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"datasets/optotaxis\")\n",
    "\n",
    "imfolder = data_path/\"images\";#'C:\\\\Users\\\\miner\\\\OneDrive - University of North Carolina at Chapel Hill\\\\Bear Lab\\\\optotaxis calibration\\\\data\\\\segmentation_iteration_testing\\\\iter4\\\\round1\\\\images'\n",
    "maskfolder = data_path/\"masks\";#Path('C:/Users/miner/OneDrive - University of North Carolina at Chapel Hill/Bear Lab/optotaxis calibration/data/segmentation_iteration_testing/iter4/round1/masks')\n",
    "imp = ImageSegmentationDirectoryImporter(data_path=imfolder,labels_path=maskfolder)\n",
    "dataset = fo.Dataset.from_importer(imp)\n",
    "\n",
    "import fiftyone.utils.random as four\n",
    "\n",
    "four.random_split(dataset, {\"train\": 0.8, \"val\": 0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ceff1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        2025.04.20.22.40.30\n",
       "Media type:  image\n",
       "Num samples: 56\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Segmentation)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0eda62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from imageio.v3 import imread\n",
    "import numpy as np\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "meanstdpath = data_path/\"meanstd.json\"\n",
    "if meanstdpath.exists():\n",
    "    with open(meanstdpath,\"r\") as f:\n",
    "        MEAN,STD = json.load(f)\n",
    "else:\n",
    "    from running_stats import RunningStats\n",
    "    def get_im_mean_std(dataset:fo.Dataset):\n",
    "        stats = RunningStats(n=np.array(0),m=np.array(0),s=np.array(0))\n",
    "        for samp in dataset.iter_samples(progress=True):\n",
    "            im = imread(samp.filepath)\n",
    "\n",
    "            #image transforms - obviated by segmentation pipeline\n",
    "            im = rescale_intensity(im)\n",
    "            if im.dtype != np.uint8:\n",
    "                im = np.astype(im/255,np.uint8)\n",
    "            \n",
    "            stats += im;\n",
    "\n",
    "        return stats.mean, stats.std\n",
    "    MEAN,STD = get_im_mean_std(dataset)\n",
    "    print(MEAN,STD)\n",
    "    meanstdpath = data_path/\"meanstd.json\"\n",
    "    with open(meanstdpath,\"w\") as f:\n",
    "        json.dump((MEAN,STD),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from imageio.v3 import imread\n",
    "# from bidict import bidict\n",
    "\n",
    "\n",
    "class FOTorchSegmentationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset containing segmantic segmentation masks.\n",
    "    \n",
    "    Args:\n",
    "        fiftyone_dataset: a FiftyOne dataset or view that will be used for \n",
    "            training or testing\n",
    "        transforms (None): a list of PyTorch transforms to apply to images \n",
    "            and targets when loading\n",
    "        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset \n",
    "            that contains the desired labels to load\n",
    "        classes (None): a list of class strings that are used to define the \n",
    "            mapping between class names and indices. If None, it will use \n",
    "            all classes present in the given fiftyone_dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fiftyone_dataset:fo.Dataset,\n",
    "        classes:list[str]|str,\n",
    "        transforms=None,\n",
    "        gt_field=\"ground_truth\",\n",
    "    ):\n",
    "        self.samples = fiftyone_dataset\n",
    "        self.transforms = transforms\n",
    "        self.gt_field = gt_field\n",
    "\n",
    "        self.img_paths = self.samples.values(\"filepath\")\n",
    "        self.mask_paths = self.samples.values(f\"{gt_field}.mask_path\")\n",
    "        self.classes = [classes] if isinstance(classes,str) else classes\n",
    "\n",
    "        if self.classes[0] != \"background\":\n",
    "            self.classes = [\"background\"] + self.classes\n",
    "\n",
    "        self.labels_map = {i: c for i, c in enumerate(self.classes)}\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = imread(img_path)\n",
    "\n",
    "        img = np.stack([img,img,img],axis=-1)\n",
    "\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        mask = imread(mask_path)\n",
    "        mask = mask > 0 #reads mask 0,255 otherwise\n",
    "\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            t = self.transforms(image=img, mask=mask)\n",
    "            timg = t[\"image\"]\n",
    "            tmask = t[\"mask\"]\n",
    "\n",
    "        return timg, tmask, img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c7f532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miner\\miniconda3\\envs\\590\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "transform = A.Normalize(mean=MEAN, std=STD)\n",
    "train_dataset = FOTorchSegmentationDataset(dataset.match_tags(\"train\"),classes=\"cytoplasm\",transforms=transform)\n",
    "test_dataset = FOTorchSegmentationDataset(dataset.match_tags(\"val\"),classes=\"cytoplasm\",transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d927ce09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db22d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn training dataset into something mask2former can see\n",
    "from transformers import MaskFormerImageProcessor\n",
    "\n",
    "# Create a preprocessor\n",
    "mask2processor = MaskFormerImageProcessor(reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n",
    "\n",
    "##mask2PROCESSOR USAGE: Call the preprocessor on a *batch* of images and segmentation maps. Images are RGB, masks are segmentation #s.\n",
    "# eg. mask2processor(image_batch,mask_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbe7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    images = inputs[0]\n",
    "    segmentation_maps = inputs[1]\n",
    "    # this function pads the inputs to the same size,\n",
    "    # and creates a pixel mask\n",
    "    # actually padding isn't required here since we are cropping\n",
    "    # print(np.array(images).shape)\n",
    "    batch = mask2processor( #creates a dict with various inputs to the model. Key (get it) arguments: \"pixel_values\" - raw image input. \"mask_labels\" - per-class binary masks. \"class_labels\" - integer label per mask\n",
    "        images,\n",
    "        segmentation_maps=segmentation_maps,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    batch[\"original_images\"] = inputs[2]\n",
    "    batch[\"original_segmentation_maps\"] = inputs[3]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a2efc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MaskFormerForInstanceSegmentation were not initialized from the model checkpoint at facebook/maskformer-swin-base-ade and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MaskFormerForInstanceSegmentation\n",
    "model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-ade\",\n",
    "                                                          id2label=train_dataset.labels_map,\n",
    "                                                          ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9e06075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a2f564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Fine-Tuning Loop (Semantic)\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "debug_cpu = False\n",
    "device = torch.device(\"cuda\" if not debug_cpu and torch.cuda.is_available() else \"cpu\")\n",
    "print(\"torch device:\",device)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "running_loss = 0.0\n",
    "num_samples = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4618cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1711831092834473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [09:56<00:00, 25.94s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]c:\\Users\\miner\\miniconda3\\envs\\590\\lib\\site-packages\\datasets\\features\\image.py:347: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
      "100%|██████████| 6/6 [00:28<00:00,  4.77s/it]\n",
      "C:\\Users\\miner\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0\\mean_iou.py:153: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
      "  area_label = np.histogram(label, bins=num_labels, range=(0, num_labels - 1))[0]\n",
      "C:\\Users\\miner\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0\\mean_iou.py:258: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  all_acc = total_area_intersect.sum() / total_area_label.sum()\n",
      "C:\\Users\\miner\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0\\mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  iou = total_area_intersect / total_area_union\n",
      "C:\\Users\\miner\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0\\mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n",
      "C:\\Users\\miner\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0\\mean_iou.py:262: RuntimeWarning: Mean of empty slice\n",
      "  metrics[\"mean_iou\"] = np.nanmean(iou)\n",
      "C:\\Users\\miner\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0\\mean_iou.py:263: RuntimeWarning: Mean of empty slice\n",
      "  metrics[\"mean_accuracy\"] = np.nanmean(acc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU: nan\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/23 [00:26<09:39, 26.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5246903249557983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/23 [00:53<19:36, 53.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Backward propagation\u001b[39;00m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     23\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\miner\\miniconda3\\envs\\590\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\miner\\miniconda3\\envs\\590\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\miner\\miniconda3\\envs\\590\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(100)):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  model.train()\n",
    "  for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "      # Reset the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # print(\"mask:\",batch[\"mask_labels\"])\n",
    "      # print(\"class:\",batch[\"class_labels\"])\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          pixel_values=batch[\"pixel_values\"].to(device,torch.float),\n",
    "          mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "          class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "      )\n",
    "\n",
    "      # Backward propagation\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].size(0)\n",
    "      running_loss += loss.item()\n",
    "      num_samples += batch_size\n",
    "\n",
    "      if idx % 100 == 0:\n",
    "        print(\"Loss:\", running_loss/num_samples)\n",
    "\n",
    "      # Optimization\n",
    "      optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "    if idx > 5:\n",
    "      break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values.to(device))\n",
    "\n",
    "    # get original images\n",
    "    original_images = batch[\"original_images\"]\n",
    "    target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "    # predict segmentation maps\n",
    "    predicted_segmentation_maps = mask2processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                  target_sizes=target_sizes)\n",
    "\n",
    "    # get ground truth segmentation maps\n",
    "    ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "    metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "  \n",
    "  # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n",
    "  # so if you're interested, feel free to print them as well\n",
    "  print(\"Mean IoU:\", metric.compute(num_labels = 1, ignore_index = 0)['mean_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e21e37df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]c:\\Users\\miner\\miniconda3\\envs\\590\\lib\\site-packages\\datasets\\features\\image.py:347: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
      "100%|██████████| 6/6 [00:33<00:00,  5.57s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "    if idx > 5:\n",
    "      break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values.to(device,torch.float))\n",
    "\n",
    "    # get original images\n",
    "    original_images = batch[\"original_images\"]\n",
    "    target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "    # predict segmentation maps\n",
    "    predicted_segmentation_maps = mask2processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                  target_sizes=target_sizes)\n",
    "\n",
    "    # get ground truth segmentation maps\n",
    "    ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "    metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "871dc883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miner\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0\\mean_iou.py:153: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
      "  area_label = np.histogram(label, bins=num_labels, range=(0, num_labels - 1))[0]\n",
      "C:\\Users\\miner\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0\\mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n",
      "  iou = total_area_intersect / total_area_union\n",
      "C:\\Users\\miner\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--mean_iou\\9e450724f21f05592bfb0255fe2fa576df8171fa060d11121d8aecfff0db80d0\\mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n",
      "  acc = total_area_intersect / total_area_label\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean IoU:\", metric.compute(num_labels = 2, ignore_index = 0)['mean_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d69b43ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(batch[\"original_segmentation_maps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "997721f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_segmentation_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4130c3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(predicted_segmentation_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0d973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "590",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
